{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### insert database\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://caochanhduong:bikhungha1@ds261626.mlab.com:61626/activity?retryWrites=false')\n",
    "db = client.activity\n",
    "activities = db.activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_data/real_db_494_final.json','r') as db_file:\n",
    "    list_data = json.load(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for data in list_data:\n",
    "    data[\"time_works_place_address_mapping\"] = []\n",
    "    del data[\"time_work_place_mapping\"]\n",
    "    del data[\"_id\"]\n",
    "    activities.insert_one(data)\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name_activity', 'type_activity', 'holder', 'time', 'name_place', 'address', 'reward', 'contact', 'register', 'works', 'joiner'])\n",
      "------------------------type activity\n",
      "tuyển hỗ trợ viên\n",
      "hỗ trợ chương trình\n",
      "hỗ trợ triễn lãm\n",
      "buổi hỗ trợ\n",
      "tuyển ctv hỗ trợ\n",
      "hỗ trợ\n",
      "hỗ trợ gây quỹ\n"
     ]
    }
   ],
   "source": [
    "with open('real_dict_2000_new_only_delete_question_noti_new_and_space_newest.json','r') as dict_file:\n",
    "    real_dict = json.load(dict_file)\n",
    "    print(real_dict.keys())\n",
    "    list_holder = real_dict['holder']\n",
    "    list_name_place = real_dict['name_place']\n",
    "    list_name_activity = real_dict['name_activity']\n",
    "    list_joiner = real_dict['joiner']\n",
    "    list_type_activity = real_dict['type_activity']\n",
    "    list_reward = real_dict['reward']\n",
    "    list_works = real_dict['works']\n",
    "    \n",
    "    res = []\n",
    "\n",
    "#     print(\"------------------------joiner\")\n",
    "#     for joiner in list_joiner:\n",
    "#         if compound2unicode(\"bạn\") in compound2unicode(joiner):\n",
    "#             print(joiner)\n",
    "#     print(\"------------------------works\")\n",
    "#     for works in list_works:\n",
    "#         if compound2unicode(\"hoạt động\") in compound2unicode(works):\n",
    "#             print(works)\n",
    "            \n",
    "#     print(\"------------------------reward\")\n",
    "#     for reward in list_reward:\n",
    "# #         if \"khoa học và công nghệ\" in reward:\n",
    "#         print(reward)\n",
    "            \n",
    "    print(\"------------------------type activity\")\n",
    "    for type_activity in list_type_activity:\n",
    "        if compound2unicode(\"hỗ trợ\") in compound2unicode(type_activity):\n",
    "            print(type_activity)\n",
    "            \n",
    "#     print(\"------------------------holder\")\n",
    "#     for holder in list_holder:\n",
    "#         if compound2unicode(\"bách khoa\") in compound2unicode(holder):\n",
    "#             print(holder)\n",
    "            \n",
    "#     print(\"------------------------name_place\")\n",
    "#     for name_place in list_name_place:\n",
    "#         if compound2unicode(\"bách khoa\") in compound2unicode(name_place):\n",
    "#             print(name_place)\n",
    "#     print(\"------------------------name_activity\")\n",
    "#     for name_activity in list_name_activity:\n",
    "#         if \"bách khoa\" in name_activity:\n",
    "#             print(name_activity)\n",
    "#     for joiner in list_joiner:\n",
    "#         print(joiner)\n",
    "#         if name_activity.find(\"chương trình\") == 0:\n",
    "#             res.append(name_activity.replace(\"chương trình \",\"\"))\n",
    "#         else:\n",
    "#             res.append(name_activity)\n",
    "# #     print(res)\n",
    "#     real_dict['name_activity'] = res\n",
    "#     with open('real_dict_2000_new_only_delete_question_noti_new_and_space_newest.json', 'w+') as new_dict_file:\n",
    "#         json.dump(real_dict,new_dict_file,ensure_ascii=False)\n",
    "# print(list_holder)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n"
     ]
    }
   ],
   "source": [
    "with open('temp_data/real_db_769.json','r') as db_file:\n",
    "    list_data = json.load(db_file)\n",
    "#     print(list_data)\n",
    "    res = list_data\n",
    "    for data in list_data:\n",
    "        check_not_match_holder = False\n",
    "#         if 'name_activity' not in list(data.keys()):\n",
    "#             break\n",
    "        for holder in data['holder']:\n",
    "            if holder not in list_holder:\n",
    "                check_not_match_holder = True\n",
    "                break\n",
    "        if check_not_match_holder == True:\n",
    "            res.remove(data)\n",
    "            \n",
    "    for data in list_data:\n",
    "        check_not_match_name_place = False\n",
    "#         if 'name_activity' not in list(data.keys()):\n",
    "#             break\n",
    "        for name_place in data['name_place']:\n",
    "            if name_place not in list_name_place:\n",
    "                check_not_match_name_place = True\n",
    "                break\n",
    "        if check_not_match_name_place == True:\n",
    "            res.remove(data)\n",
    "    \n",
    "    \n",
    "    print(len(res))\n",
    "    with open('temp_data/real_db_494.json','w+') as new_db_file:\n",
    "        json.dump(res,new_db_file,ensure_ascii=False)\n",
    "    \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\d\\d\\d\n"
     ]
    }
   ],
   "source": [
    "with open('list_constants.json','r') as constants_file:\n",
    "    list_constants = json.load(constants_file)\n",
    "    list_pattern_time = list_constants['list_pattern_time']\n",
    "    for pattern in list_pattern_time:\n",
    "        if re.findall(pattern,'305') != []:\n",
    "            print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp_agent_action_gen import *\n",
    "from message_handler import *\n",
    "from agen_response_gen import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entity(intent,input_sentence):\n",
    "    print(\"duongcc\")\n",
    "    normalized_input_sentence = compound2unicode(input_sentence)\n",
    "    normalized_input_sentence = delete_extra_word(normalized_input_sentence,list_extra_word)\n",
    "    \n",
    "    result_entity_dict={}\n",
    "    list_order_entity_name=map_intent_to_list_order_entity_name[intent]\n",
    "    print(normalized_input_sentence)\n",
    "    if 'time' in list_order_entity_name:\n",
    "        for pattern_time in list_pattern_time:\n",
    "            if re.findall(pattern_time,normalized_input_sentence)!=[]:\n",
    "                # print(\"pattern_time :{0}\".format(pattern_time))\n",
    "                if 'time' not in result_entity_dict:\n",
    "                    result_entity_dict['time'] = delete_last_space_list(re.findall(pattern_time,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['time'].extend(delete_last_space_list(re.findall(pattern_time,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_time,' '.join(['✪']*(pattern_time.count(' ')+1)),normalized_input_sentence)\n",
    "        # if 'time' in result_entity_dict:\n",
    "        #     print(result_entity_dict['time'])\n",
    "    if 'reward' in list_order_entity_name:\n",
    "        for pattern_reward in list_pattern_reward:\n",
    "            if re.findall(pattern_reward,normalized_input_sentence)!=[]:\n",
    "                print(\"pattern_reward :{0}\".format(pattern_reward))\n",
    "                if 'reward' not in result_entity_dict:\n",
    "                    result_entity_dict['reward'] = delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['reward'].extend(delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_reward,' '.join(['✪']*(pattern_reward.count(' ')+1)),normalized_input_sentence)\n",
    "        # if 'reward' in result_entity_dict:\n",
    "        #     print(result_entity_dict['reward'])\n",
    "    matching_threshold = 0.0\n",
    "    longest_common_length, end_common_index = None, None\n",
    "    \n",
    "    map_entity_name_to_threshold={}\n",
    "    for entity_name in list_order_entity_name:\n",
    "        if entity_name in ['time','address']:\n",
    "            map_entity_name_to_threshold[entity_name]=1\n",
    "        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['works','reward']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['register']:\n",
    "            map_entity_name_to_threshold[entity_name]=3\n",
    "\n",
    "\n",
    "    ordered_real_dict = OrderedDict()\n",
    "    for entity_name in map_intent_to_list_order_entity_name[intent]:\n",
    "        ordered_real_dict[entity_name] = real_dict[entity_name]\n",
    "    for entity_name, list_entity in ordered_real_dict.items():\n",
    "        # print(entity_name)\n",
    "        list_entity = [entity.lower() for entity in list_entity]\n",
    "        # print(\"input sentence: {0}\".format(normalized_input_sentence))\n",
    "        if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "            matching_threshold = 0.15\n",
    "        elif entity_name == \"joiner\":\n",
    "            matching_threshold = 0.2\n",
    "        else:\n",
    "            matching_threshold = 0.5\n",
    "        print(\"000. sentence:{0}\".format(normalized_input_sentence))\n",
    "        catch_entity_threshold_loop = 0\n",
    "        while True:\n",
    "            if catch_entity_threshold_loop > 5:\n",
    "                break\n",
    "            list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity,entity_name)\n",
    "#             print(list_dict_longest_common_entity)\n",
    "                #     [{'longest_common_entity_index': 0,\n",
    "                #   'longest_common_length': 3,\n",
    "                #   'end_common_index': 9}]\n",
    "            \n",
    "\n",
    "            ##find the most match longest common match (calculate by length of token match in sentence \n",
    "                                                                #/ length of entity )\n",
    "                            ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}\n",
    "            if list_dict_longest_common_entity == []:\n",
    "                break\n",
    "            if list_dict_longest_common_entity[0]['longest_common_length'] < map_entity_name_to_threshold[entity_name] :\n",
    "                break\n",
    "            \n",
    "            list_sentence_token = normalized_input_sentence.split(' ')\n",
    "#             print(\"list_sentence_token :{0}\".format(list_sentence_token))\n",
    "            greatest_entity_index=None\n",
    "            greatest_common_length = None\n",
    "            greatest_end_common_index = None\n",
    "            max_match_entity = 0.0\n",
    "#             print(\"common entity :{0}\".format(list_dict_longest_common_entity))\n",
    "            for dict_longest_common_entity in list_dict_longest_common_entity:\n",
    "#                 print(\"0. dict_longest_common_entity: {0}\".format(dict_longest_common_entity))\n",
    "\n",
    "#                     print(\"duong\")\n",
    "#                 print(\"0.1 entity: {0}\".format(list_entity[dict_longest_common_entity['longest_common_entity_index']]))\n",
    "                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']\n",
    "                longest_common_length = dict_longest_common_entity['longest_common_length']\n",
    "                end_common_index = dict_longest_common_entity['end_common_index']\n",
    "                \n",
    "                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length+1:end_common_index+1]\n",
    "                if entity_name == \"type_activity\":\n",
    "                    if \"ban chỉ huy\" in normalized_input_sentence or \"ban tổ chức\" in normalized_input_sentence or \"bch\" in normalized_input_sentence or \"btc\" in normalized_input_sentence:\n",
    "                        continue\n",
    "                    #nếu chỉ là các câu inform 1 entity mà câu đó không phải là câu inform tên 1 hoạt động thì không cần xét\n",
    "                    if \"inform\" not in intent or \"name_activity\" in intent:\n",
    "                        list_name_activity = ordered_real_dict['name_activity']\n",
    "                        check_in_name = False\n",
    "                        for name_activity in list_name_activity:\n",
    "                            #nếu loại hoạt động nằm trọn trong bất kì 1 tên hoạt động \n",
    "                            # thì không lấy\n",
    "                            if  name_activity.find(' '.join(list_sentence_token_match)) > 0:\n",
    "                                check_in_name = True\n",
    "                                break\n",
    "                        if check_in_name == True:\n",
    "                            continue\n",
    "                \n",
    "                if entity_name == \"holder\":\n",
    "                    # nếu holder mà trước đó có từ chỉ nơi chốn : ở, tại => không là holder mà là  \n",
    "                    # name_place\n",
    "                    if end_common_index - longest_common_length >= 0:\n",
    "                        if list_sentence_token[end_common_index - longest_common_length] in [\"ở\",\"tại\",\"trước\",\"sau\",\"trong\"]:\n",
    "                            if 'name_place' in result_entity_dict:\n",
    "            #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                                result_entity_dict['name_place'].append(' '.join(list_sentence_token_match))\n",
    "                            else:\n",
    "            #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                                result_entity_dict['name_place'] = [' '.join(list_sentence_token_match)]\n",
    "                            list_sentence_token[end_common_index - longest_common_length +1 :end_common_index +1] = [\"✪\"]*longest_common_length\n",
    "                            normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "                            continue\n",
    "\n",
    "#                 print(\"2. list_sentence_token_match : {0}\".format(list_sentence_token_match))\n",
    "                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')\n",
    "#                 print(\"3. list_temp_longest_entity_token : {0}\".format(list_temp_longest_entity_token))\n",
    "                if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "#                     print(\"list_temp_longest_entity_token :{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"list_sentence_token_match :{0}\".format(list_sentence_token_match))\n",
    "                    _,longest_common_length_entity,end_common_index_entity = lcs_length_ta(list_temp_longest_entity_token,list_sentence_token_match)\n",
    "                    list_entity_token_match = list_temp_longest_entity_token[end_common_index_entity - longest_common_length_entity +1 :end_common_index_entity +1]\n",
    "                    score = len(list_entity_token_match)/float(len(list_temp_longest_entity_token))\n",
    "#                     print(\"list_entity_token_match: {0}\".format(list_entity_token_match))\n",
    "#                     print(\"list_temp_longest_entity_token:{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"score :{0}\".format(score))\n",
    "                    \n",
    "                else:\n",
    "                    score = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                if score > max_match_entity:\n",
    "#                     max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                    max_match_entity = score\n",
    "                    greatest_entity_index = longest_common_entity_index\n",
    "                    greatest_common_length = longest_common_length\n",
    "                    greatest_end_common_index = end_common_index\n",
    "#             print(list_sentence_token)\n",
    "#             print(greatest_common_length)\n",
    "#             print(greatest_end_common_index)\n",
    "#             print(\"longest_common_length: {0}\".format(longest_common_length))\n",
    "#             print(\"end_common_index: {0}\".format(end_common_index))\n",
    "#             print(\"1. greatest_common_length : {0}\".format(greatest_common_length))\n",
    "            # print(max_match_entity)\n",
    "            # print(\"2. greatest entity : {0}\".format(list_entity[greatest_entity_index]))\n",
    "#             print(\"2.1 greatest_end_common_index: {0}\".format(greatest_end_common_index))\n",
    "#             print(\"3. sentence match: {0}\".format(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1]))\n",
    "            if greatest_common_length != None:\n",
    "                if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > matching_threshold:\n",
    "                    # if entity_name in ['name_activity','type_activity']:\n",
    "                    #     result = list_entity[greatest_entity_index]\n",
    "                    # else:\n",
    "                    #     result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                    \n",
    "                    result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                    if entity_name in result_entity_dict:\n",
    "    #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                        result_entity_dict[entity_name].append(result)\n",
    "                    else:\n",
    "    #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                        result_entity_dict[entity_name] = [result]\n",
    "    #                 list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length + 1] + list_sentence_token[greatest_end_common_index +1 :]\n",
    "                    list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1] = [\"✪\"]*greatest_common_length\n",
    "                    normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "            catch_entity_threshold_loop = catch_entity_threshold_loop + 1\n",
    "            # print(\"output sentence: {0}\".format(normalized_input_sentence))\n",
    "    return result_entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✪ ✪ ✪ diễn ra hiến máu tình nguyện đợt ii ở hội trường lớn đh ngân hàng vậy\n",
      "{'intent': 'request', 'inform_slots': {'time': ['hội trường'], 'name_activity': ['hiến máu tình nguyện đợt ii'], 'holder': ['đh ngân hàng']}, 'request_slots': {'time': 'UNK'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "CONSTANT_FILE_PATH = 'constants.json'\n",
    "with open(CONSTANT_FILE_PATH) as f:\n",
    "    constants = json.load(f)\n",
    "\n",
    "file_path_dict = constants['db_file_paths']\n",
    "DATABASE_FILE_PATH = file_path_dict['database']\n",
    "\n",
    "database= json.load(open(DATABASE_FILE_PATH,encoding='utf-8'))\n",
    "state_tracker = StateTracker(database, constants)\n",
    "dqn_agent = DQNAgent(state_tracker.get_state_size(), constants)\n",
    "#TEST\n",
    "if __name__ == '__main__':\n",
    "    print(process_message_to_user_request(\"khi nào thì diễn ra hiến máu tình nguyện đợt ii ở hội trường lớn đh ngân hàng vậy\",state_tracker))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('list_constants.json','r') as constant_file:\n",
    "#     constant = json.load(constant_file)\n",
    "#     list_pattern_time = constant['list_pattern_time']\n",
    "#     res = []\n",
    "#     for pattern in list_pattern_time:\n",
    "#         if \"chủ nhật\" not in pattern:\n",
    "#             res.append(pattern)\n",
    "            \n",
    "#     constant['list_pattern_time'] = res\n",
    "#     with open('list_constants_new.json', 'w+') as new_constant_file:\n",
    "#         json.dump(constant,new_constant_file,ensure_ascii=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_data/real_db_494.json','r') as db_file:\n",
    "    list_data = json.load(db_file)\n",
    "    for data in list_data:\n",
    "        for key, list_value in data.items():\n",
    "            if key != '_id':\n",
    "                list_value = [compound2unicode(value).lower() for value in list_value]\n",
    "                data[key] = list(set(list_value))\n",
    "    with open('temp_data/real_db_494_final.json','w+') as new_db_file:\n",
    "        json.dump(list_data,new_db_file,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongCao",
   "language": "python",
   "name": "duongcao2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
